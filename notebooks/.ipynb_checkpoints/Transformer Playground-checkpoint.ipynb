{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots to discover when using the transformer decoder model on non-text data. A running list of things to figure out include:\n",
    "\n",
    "- input non text?\n",
    "- no encoder ?\n",
    "- causal prediction of prices and/or pct_change at end of sequence?\n",
    "\n",
    "- positional encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together...\n",
    "\n",
    "pasting the only used functions/classes and then doing a toy forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def initialize_weight(x):\n",
    "    nn.init.xavier_uniform_(x.weight)\n",
    "    if x.bias is not None:\n",
    "        nn.init.constant_(x.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate,output_size=None):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        if output_size:\n",
    "            self.layer2 = nn.Linear(filter_size, output_size)\n",
    "        else:\n",
    "            self.layer2 = nn.Linear(filter_size, hidden_size)\n",
    "\n",
    "        initialize_weight(self.layer1)\n",
    "        initialize_weight(self.layer2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_rate, head_size=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.att_size = att_size = hidden_size // head_size\n",
    "        self.scale = att_size ** -0.5\n",
    "\n",
    "        self.linear_q = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        self.linear_k = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        self.linear_v = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        initialize_weight(self.linear_q)\n",
    "        initialize_weight(self.linear_k)\n",
    "        initialize_weight(self.linear_v)\n",
    "\n",
    "        self.att_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.output_layer = nn.Linear(head_size * att_size, hidden_size,\n",
    "                                      bias=False)\n",
    "        initialize_weight(self.output_layer)\n",
    "\n",
    "    def forward(self, q, k, v, mask, cache=None):\n",
    "        orig_q_size = q.size()\n",
    "\n",
    "        d_k = self.att_size\n",
    "        d_v = self.att_size\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
    "        q = self.linear_q(q).view(batch_size, -1, self.head_size, d_k)\n",
    "        if cache is not None and 'encdec_k' in cache:\n",
    "            k, v = cache['encdec_k'], cache['encdec_v']\n",
    "        else:\n",
    "            k = self.linear_k(k).view(batch_size, -1, self.head_size, d_k)\n",
    "            v = self.linear_v(v).view(batch_size, -1, self.head_size, d_v)\n",
    "\n",
    "            if cache is not None:\n",
    "                cache['encdec_k'], cache['encdec_v'] = k, v\n",
    "\n",
    "        q = q.transpose(1, 2)                  # [b, h, q_len, d_k]\n",
    "        v = v.transpose(1, 2)                  # [b, h, v_len, d_v]\n",
    "        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len]\n",
    "\n",
    "        # Scaled Dot-Product Attention.\n",
    "        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
    "        q.mul_(self.scale)\n",
    "        x = torch.matmul(q, k)  # [b, h, q_len, k_len]\n",
    "        x.masked_fill_(mask.unsqueeze(1), -1e9)\n",
    "        x = torch.softmax(x, dim=3)\n",
    "        x = self.att_dropout(x)\n",
    "        x = x.matmul(v)  # [b, h, q_len, attn]\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn]\n",
    "        x = x.view(batch_size, -1, self.head_size * d_v)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        assert x.size() == orig_q_size\n",
    "        return x\n",
    "\n",
    "\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "       # self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        #self.enc_dec_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        #self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, self_mask)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "       # if enc_output is not None:\n",
    "       #     y = self.enc_dec_attention_norm(x)\n",
    "        #    y = self.enc_dec_attention(y, enc_output, enc_output, i_mask,\n",
    "                                   #    cache)\n",
    "          #  y = self.enc_dec_attention_dropout(y)\n",
    "        #   x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        decoders = [DecoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(decoders)\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        decoder_output = x\n",
    "        for i, dec_layer in enumerate(self.layers):\n",
    "            decoder_output = dec_layer(decoder_output,\n",
    "                                       self_mask)\n",
    "        return self.last_norm(decoder_output)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_layers=6,\n",
    "                 hidden_size=32,\n",
    "                 filter_size=64,\n",
    "                 dropout_rate=0.1,\n",
    "                window_size=32):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_scale = hidden_size ** 0.5\n",
    "        \n",
    "        \n",
    "\n",
    "        self.price_embedding = nn.Linear(window_size,hidden_size) # this 1 can be scaled to # of cryptos, trends, etc. Need to fix loaders as well for that but interesting opportunity. \n",
    "        \n",
    "        nn.init.normal_(self.price_embedding.weight, mean=0,\n",
    "                        std=hidden_size**-0.5)\n",
    "        \n",
    "        self.decoder = Decoder(hidden_size, filter_size,\n",
    "                               dropout_rate, n_layers)\n",
    "        \n",
    "        self.output_mlp = FeedForwardNetwork(hidden_size*hidden_size,filter_size,dropout_rate,output_size=1) \n",
    "\n",
    "\n",
    "        # For positional encoding\n",
    "        num_timescales = self.hidden_size // 2\n",
    "        max_timescale = 10000.0\n",
    "        min_timescale = 1.0\n",
    "        log_timescale_increment = (\n",
    "            math.log(float(max_timescale) / float(min_timescale)) /\n",
    "            max(num_timescales - 1, 1))\n",
    "        inv_timescales = min_timescale * torch.exp(\n",
    "            torch.arange(num_timescales, dtype=torch.float32) *\n",
    "            -log_timescale_increment)\n",
    "        self.register_buffer('inv_timescales', inv_timescales)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        self_mask = create_self_mask(self.window_size)\n",
    "        \n",
    "        \n",
    "        x = self.decode(x, self_mask)\n",
    "       # print(x.shape)\n",
    "        x = self.output_mlp(x.flatten(start_dim=1))\n",
    "        return x\n",
    "\n",
    "\n",
    "    def decode(self, x, self_mask):\n",
    "\n",
    "       # print(x.shape)\n",
    "      #  x = self.price_embedding(x).unsqueeze(0)\n",
    "      #  print(y.shape)\n",
    "        #x = x[:,0] * torch.eye(self.window_size) # this does not convert to batch size \n",
    "\n",
    "     #   print(x.shape)\n",
    "        x = torch.stack([x[i,:] * torch.eye(x.shape[1]) for i in range(x.shape[0])])\n",
    "        #x = x.unsqueeze(0)\n",
    "       # print(x.shape)\n",
    "\n",
    "        x *= self.emb_scale\n",
    "        x += self.get_position_encoding(x)\n",
    "      #  print(x.shape)\n",
    "     #   target_embedded = self.t_emb_dropout(target_embedded)\n",
    "\n",
    "        # decoder\n",
    "        output = self.decoder(x, self_mask)\n",
    "\n",
    "        return output # and take the last token of this.. \n",
    "\n",
    "    def get_position_encoding(self, x):\n",
    "        max_length = x.size()[1]\n",
    "        position = torch.arange(max_length, dtype=torch.float32,\n",
    "                                device=x.device)\n",
    "        scaled_time = position.unsqueeze(1) * self.inv_timescales.unsqueeze(0)\n",
    "        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)],\n",
    "                           dim=1)\n",
    "        signal = F.pad(signal, (0, 0, 0, self.hidden_size % 2))\n",
    "        signal = signal.view(1, max_length, self.hidden_size)\n",
    "        return signal\n",
    "\n",
    "\n",
    "def create_self_mask(target_len, device=None):\n",
    "    # Prevent leftward information flow in self-attention.\n",
    "    ones = torch.ones(target_len, target_len, dtype=torch.uint8,\n",
    "                      device=device)\n",
    "    t_self_mask = torch.triu(ones, diagonal=1).unsqueeze(0)\n",
    "\n",
    "    return t_self_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what this all looks like, for the future model(s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from dataloaders import create_dataloaders\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_loader, *_ = create_dataloaders(prior_years=8,prior_days=0,pct_window=1,crypto='bitcoin',values='usd',batch_size=4,buy_thresh=3,labels_to_load=['pct_change'],window=14)\n",
    "\n",
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(hidden_size=14,window_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-6c9a0e437864>:85: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1104.)\n",
      "  x.masked_fill_(mask.unsqueeze(1), -1e9)\n"
     ]
    }
   ],
   "source": [
    "out = model.forward( batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6800],\n",
       "        [0.7011],\n",
       "        [1.3353],\n",
       "        [2.3699]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass this output to a little NN, and the rest is history. That'll do. \n",
    "\n",
    "One thought is maybe make the hidden size larger that way days can more effectively occupy separate areas? or do we like the blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Embedding?\n",
    " to use this, I would need all possible integer values this price can take, which is a lot! Instead, we skip the lookup table and transform to a \n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
