{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for preparing and testing lightning data and trainer modules to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the data module, which will be pretty easy as I made the loaders in a different spot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_dataloaders() missing 2 required positional arguments: 'prior_days' and 'pct_window'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2f54ecddff21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_years\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcrypto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bitcoin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'usd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbuy_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_to_load\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pct_change'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_dataloaders() missing 2 required positional arguments: 'prior_days' and 'pct_window'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from dataloaders import create_dataloaders\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_loader, _, _ = create_dataloaders(prior_years=3,crypto='bitcoin',values='usd',batch_size=8,buy_thresh=3,labels_to_load=['pct_change'],window=7)\n",
    "\n",
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 crypto: str = \"bitcoin\", \n",
    "                 prior_years: int = 5,\n",
    "                 values: str = 'usd',\n",
    "                 buy_thresh: int = 3,\n",
    "                 labels_to_load: list = ['pct_change'],\n",
    "                 window: int = 14,\n",
    "                 batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.crypto = crypto\n",
    "        self.prior_years = prior_years\n",
    "        self.values = values\n",
    "        self.buy_thresh = buy_thresh\n",
    "        self.labels_to_load = labels_to_load.split(',')\n",
    "        self.window = window\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_loader, self.val_loader, self.test_loader = create_dataloaders(prior_years=self.prior_years,\n",
    "                                                                                  crypto=self.crypto,\n",
    "                                                                                  values=self.values,\n",
    "                                                                                  batch_size=self.batch_size,\n",
    "                                                                                  buy_thresh=self.buy_thresh,\n",
    "                                                                                  labels_to_load=self.labels_to_load,\n",
    "                                                                                  window=self.window)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5399c51c3c78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCryptoTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"\n\u001b[1;32m      3\u001b[0m     \"\"\"\n\u001b[1;32m      4\u001b[0m     def __init__(self,args,\n\u001b[1;32m      5\u001b[0m                  \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pl' is not defined"
     ]
    }
   ],
   "source": [
    "class CryptoTrainer(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self,args,\n",
    "                 n_layers,\n",
    "                 filter_size,\n",
    "                 dropout_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "    \n",
    "    \n",
    "        hidden_size = args.window_size\n",
    "    \n",
    "        self.model =  Transformer(n_layers=n_layers,\n",
    "                 hidden_size=hidden_size,\n",
    "                 filter_size=filter_size,\n",
    "                 dropout_rate=dropout_rate,\n",
    "                window_size=args.window_size)\n",
    "        \n",
    "        # todo - some wandb logging\n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self,batch, batch_idx):\n",
    "        X, y = batch\n",
    "\n",
    "        y_pred = self(X)\n",
    "\n",
    "        loss = F.mse_loss(y,y_pred)\n",
    "\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        # TODO , on every X steps maybe examine the plot and see what I think?\n",
    "        return loss\n",
    "\n",
    "    # validation step\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        X, y = batch\n",
    "\n",
    "        loss = F.mse_loss(y,y_pred)\n",
    "\n",
    "        self.log('valid_loss', loss, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    #test step\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        X, y = batch\n",
    "\n",
    "        loss = F.mse_loss(y,y_pred)\n",
    "\n",
    "        self.log('test_loss', loss, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # optimizers\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer =  torch.optim.Adam(self.parameters(), lr = self.learning_rate ,weight_decay = self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience = 4)\n",
    "        return [optimizer], [scheduler] \n",
    "\n",
    "        \n",
    "        # args\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "\n",
    "        parser.add_argument('--learning_rate', type=float, default=3e-4)\n",
    "        parser.add_argument('--n_layers', type=int,default=12) \n",
    "        parser.add_argument('--filter_size', type=int,default=12) \n",
    "        parser.add_argument('--weight_decay', type=float, default=3e-4)\n",
    "        parser.add_argument('--dropout_rate', type=float, default=.1)\n",
    "\n",
    "        \n",
    "        return parser\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shopping list of arguments to get\n",
    "\n",
    "batch size\n",
    "\n",
    "model specific ones\n",
    "\n",
    "\n",
    "window size\n",
    "\n",
    "prior years\n",
    "\n",
    "crypto\n",
    "value\n",
    "threshold \n",
    "labels to load \n",
    "\n",
    "Also need to ensure how variables are declared. Args or args.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    pl.seed_everything(42)\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int,\n",
    "                        default=32)\n",
    "    parser.add_argument('--prior_years', type=int,\n",
    "                        default=5)\n",
    "    parser.add_argument('--window_size', type=int,\n",
    "                        default=28)\n",
    "    parser.add_argument('--crypto', type=str,\n",
    "                        default=\"bitcoin\")\n",
    "    parser.add_argument('--values', type=str,\n",
    "                        default='usd')\n",
    "    parser.add_argument('--labels_to_load', type=str,\n",
    "                        default='pct_change')\n",
    "\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    parser = CryptoTrainer.add_model_specific_args(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ------------\n",
    "    # training\n",
    "    # ------------\n",
    "\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        save_last = True,\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"valid_loss\",\n",
    "    mode=\"min\"\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "                       monitor='valid_loss',\n",
    "                       min_delta=0.00,\n",
    "                       patience=30,\n",
    "                       verbose=False,\n",
    "                       mode='min'\n",
    "                    )\n",
    "\n",
    "    lr_monitor = LearningRateMonitor(logging_interval= 'step')\n",
    "\n",
    "    run = wandb.init()#entity=\"automated-reporting-fdl2021\",project=\"ESTR\")\n",
    "\n",
    "\n",
    "    wandb_logger = WandbLogger() #where to specify TODO\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer().from_argparse_args(args,logger=wandb_logger,\n",
    "                       callbacks=[checkpoint_callback,lr_monitor,early_stopping_callback])\n",
    "\n",
    "\n",
    "\n",
    "    # ------------\n",
    "    # data\n",
    "    # ------------\n",
    "    data_module = CryptoDataModule(crypto=args.crypto,\n",
    "                 prior_years=args.prior_years,\n",
    "                 values=args.values,\n",
    "                 buy_thresh,=args.buy_thresh\n",
    "                 labels_to_load=args.labels_to_load,\n",
    "                 window=args.window_size,\n",
    "                 batch_size=args.batch_size,\n",
    "            )\n",
    "\n",
    "\n",
    "    # ------------\n",
    "    # model\n",
    "    # ------------\n",
    "\n",
    "    trader = CryptoTrainer(args,\n",
    "                 n_layers=args.n_layers,\n",
    "                 filter_size=args.filter_size,\n",
    "                 dropout_rate=args.dropout_rate)\n",
    "        \n",
    "    \n",
    "    \n",
    "  #  ESTR(args,\n",
    "    #    model_name_or_path=args.model_name_or_path,\n",
    "     #   n_layers=args.n_layers,\n",
    "        learning_rate=args.learning_rate,\n",
    "        adam_beta1=args.adam_beta1,\n",
    "        adam_beta2=args.adam_beta2,\n",
    "        adam_epsilon=args.adam_epsilon,\n",
    "        weight_decay=args.weight_decay,\n",
    "        load_pretrained = args.load_pretrained,\n",
    "        num_warmup_steps = num_warmup_steps,\n",
    "        num_training_steps = num_training_steps,\n",
    "        num_cycles = num_cycles,\n",
    "                #    )\n",
    "\n",
    "\n",
    "    if args.auto_lr_find:\n",
    "        trainer.tune(trader, data_module)\n",
    "\n",
    "\n",
    "\n",
    "    trainer.fit(trader, data_module)\n",
    "\n",
    "\n",
    "    run.log({\"training_samples\" : trader.train_text_table})\n",
    "    run.log({\"valid_samples\" : trader.valid_text_table})\n",
    "    run.log({\"test_samples\" : trader.test_text_table})\n",
    "\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = CryptoDataModule()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in datamodule.train_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 14])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.btc_transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(hidden_size=14,window_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 14])\n",
      "torch.Size([32, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "out = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the CryptoTrainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse_loss(out,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(135.8867, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
